# -*- coding: utf-8 -*-
"""Audio Video Approach.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D-NUZddpi_KxK9uEkHq1q3K2a3Fhx_dt
"""


import pandas as pd
import cv2
import os
import numpy as np
from tensorflow.keras.utils import to_categorical
import keras
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input
import tensorflow as tf
import numpy as np
from tensorflow.keras.models import Sequential
from keras.layers.core import Flatten, Dense, Dropout
from keras.layers.convolutional import Convolution1D,Convolution2D, MaxPooling2D, ZeroPadding2D,MaxPooling1D,ZeroPadding1D
from tensorflow.keras.optimizers import SGD
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/MyDrive/speech emotion detection/dataset'

folders  =  list(os.listdir('dataset/Images/'))
print(folders)

for i in range(len(folders)):
    try:
        if '.' in folders[i]:
            folders.remove(folders[i])
    except IndexError:
        pass

"""# Reading Audio Images"""

audio_images = []
images_name = []
for i in range(len(folders)):
    files = os.listdir(os.getcwd() + "dataset/Images/"+folders[i])
    for j in range(len(files)):
        images_name.append(folders[i]+"/"+files[j])
    for j in range(len(files)):
        images_name.append(folders[i]+"/"+files[j])

for i in range(len(images_name)):
    #print(i)
    audio_images.append(cv2.cvtColor(cv2.imread(os.getcwd() +"dataset/Images/"+ images_name[i][:-3] +"png"), cv2.COLOR_BGR2GRAY)[42:250, 54:380])

"""# Reading Video Images"""

video_images = []
for i in range(len(images_name)):
    #print(i)
    img_path = os.getcwd() +"dataset/Video Images/Video Images/"+ images_name[i][:10]+"1"+images_name[i][11:-3] +"png"
    video_images.append(cv2.imread(img_path))

"""# Normalizing Data"""

for i in range(len(video_images)):
    #print(i)
    video_images[i] = cv2.resize(np.array(video_images[i]), (180, 320))
    video_images[i] = np.array(video_images[i]) / 255



audio_images = np.array(audio_images) / 255


y = []
df = pd.DataFrame(columns=['Modality','Vocal channel','Emotion','Emotional intensity','Statement','Repetition','Actor'], index= range(len(images_name)))
for i in range(len(images_name)):
    split_list = images_name[i].split('-')
    split_list[-1] = split_list[-1][:2] 
    split_list[0] =  split_list[0][-2:]
    df.iloc[i] = split_list
    y.append(int(images_name[i].split('-')[2]))
df.drop(columns = ['Modality','Vocal channel','Emotion'], inplace = True)

obj_cols = df.loc[:, df.dtypes == np.object].columns
for col in obj_cols:
     df[col] = pd.to_numeric(df[col])

y = pd.get_dummies(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(np.array(audio_images, dtype='float32'), np.array(y), test_size=0.20,random_state=42)
audio_images = 0

from sklearn.model_selection import train_test_split
X_train2, X_test2, y_train2, y_test2 = train_test_split(np.array(video_images, dtype='float32'), np.array(y), test_size=0.20,random_state=42)
video_images = 0


# reshaping the data

X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2],1)

from tensorflow.keras import regularizers
from tensorflow.keras.layers import Masking


#defing the model
model1 = Sequential()

model1.add(ZeroPadding2D((1,1),input_shape=np.array(X_train)[0].shape))

model1.add(Convolution2D(64, 3, 3, activation='relu'))
model1.add(MaxPooling2D((2,2),  padding='same'))

model1.add(ZeroPadding2D((1,1)))
model1.add(Convolution2D(128, 3, 3, activation='relu'))



model1.add(MaxPooling2D((2,2), padding='same'))


model1.add(Flatten())


model2 = Sequential()

model2.add(ZeroPadding2D((1,1),input_shape=X_train2[0].shape))


model2.add(ZeroPadding2D((1,1)))
model2.add(Convolution2D(32, 3, 3, activation='relu'))
model2.add(ZeroPadding2D((1,1)))
model2.add(Convolution2D(32, 3, 3, activation='relu'))
model2.add(ZeroPadding2D((1,1)))
model2.add(Convolution2D(32, 3, 3, activation='relu'))
model2.add(MaxPooling2D((2,2),  padding='same'))

model2.add(ZeroPadding2D((1,1)))
model2.add(Convolution2D(64, 3, 3, activation='relu'))
model2.add(ZeroPadding2D((1,1)))
model2.add(Convolution2D(64, 3, 3, activation='relu'))
model2.add(MaxPooling2D((2,2), padding='same'))


model2.add(Flatten())

from keras.layers.merge import concatenate
from keras.models import Model

# define multi-headed input
# concatenate merge output from each model
ensemble_outputs = [model1.output, model2.output]
merge = concatenate(ensemble_outputs)
hidden = Dense(1024, activation='relu') (merge)
hidden = Dropout(0.2) (hidden)
hidden = Dense(256, activation='relu')(hidden)
hidden = Dropout(0.2) (hidden)
hidden = Dense(64, activation='relu')(hidden)
hidden = Dropout(0.2) (hidden)
output = Dense(8, activation='softmax')(hidden)
model = Model(inputs=[model1.input, model2.input], outputs=output)
# plot graph of ensemble
# plot_model(model, show_shapes=True, to_file='model_graph.png')
# compile
# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# return model

#compiling the model

print("\n************* Traing the model ***************\n")
opt = tf.keras.optimizers.Adam()
model.compile(loss='categorical_crossentropy', optimizer=opt,metrics='accuracy')
h = model.fit([X_train,X_train2], y_train,
          epochs=50,
          verbose=1,

          validation_data=([X_test,X_test2], y_test))

print("\n************* Evaluating the model ***************\n")

plt.figure(figsize=(8,5))
plt.plot(h.history['accuracy'])
plt.plot(h.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.figure(figsize=(8,5))
plt.plot(h.history['loss'])
plt.plot(h.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

y_pred = model.predict([X_test,X_test2])
y_test2 = []
y_pred2 = []
for i in range(len(y_test)):
    y_pred2.append(np.argmax(y_pred[i]))
    y_test2.append(np.argmax(y_test[i]))

from sklearn.metrics import accuracy_score
print("Accuracy =",accuracy_score(y_pred2,y_test2) * 100)

from sklearn.metrics import classification_report,confusion_matrix

print(classification_report(y_test2,y_pred2))

import seaborn as sn
import matplotlib.pyplot as plt

df_cm = pd.DataFrame(confusion_matrix(y_pred2,y_test2), index = [i for i in ['neutral','calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']],
                  columns = [i for i in ['neutral','calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']])
plt.figure(figsize = (15,10))
sn.heatmap(df_cm, annot=True)