# -*- coding: utf-8 -*-
"""driver_plate_identify.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i185RbAdr3oLRJi_TKf-aU2NpaWXqhcQ
"""

import os
from torchvision import transforms, datasets
from PIL import Image, ImageDraw, ImageFont
import joblib
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from face_rec import FaceRecogniser
from face_extract import FaceFeaturesExtractor

DRIVER_PATH = 'Driver_dataset'
MODEL_DIR_PATH = 'model'
MODEL_PATH = 'model/face_recogniser.pkl'

exif_orientation_tag = 0x0112
exif_transpose_sequences = [  # Val  0th row  0th col
    [],  # 0    (reserved)
    [],  # 1   top      left
    [Image.FLIP_LEFT_RIGHT],  # 2   top      right
    [Image.ROTATE_180],  # 3   bottom   right
    [Image.FLIP_TOP_BOTTOM],  # 4   bottom   left
    [Image.FLIP_LEFT_RIGHT, Image.ROTATE_90],  # 5   left     top
    [Image.ROTATE_270],  # 6   right    top
    [Image.FLIP_TOP_BOTTOM, Image.ROTATE_90],  # 7   right    bottom
    [Image.ROTATE_90],  # 8   left     bottom
]


class ExifOrientationNormalize(object):
    """
    Normalizes rotation of the image based on exif orientation info (if exists.)
    """

    def __call__(self, img):
        if 'parsed_exif' in img.info and exif_orientation_tag in img.info['parsed_exif']:
            orientation = img.info['parsed_exif'][exif_orientation_tag]
            transposes = exif_transpose_sequences[orientation]
            for trans in transposes:
                img = img.transpose(trans)
        return img


class Whitening(object):
    """
    Whitens the image.
    """

    def __call__(self, img):
        mean = img.mean()
        std = img.std()
        std_adj = std.clamp(min=1.0 / (float(img.numel()) ** 0.5))
        y = (img - mean) / std_adj
        return y

# Only some parts are being used
def dataset_to_embeddings(dataset, features_extractor):
    transform = transforms.Compose([
        ExifOrientationNormalize(),
        transforms.Resize(1024)
    ])

    embeddings = []
    labels = []
    for img_path, label in dataset.samples:
        print(img_path)
        _, embedding = features_extractor(transform(Image.open(img_path).convert('RGB')))
        if embedding is None:
            print("Could not find face on {}".format(img_path))
            continue
        if embedding.shape[0] > 1:
            print("Multiple faces detected for {}, taking one with highest probability".format(img_path))
            embedding = embedding[0, :]
        embeddings.append(embedding.flatten())
        labels.append(label)

    return np.stack(embeddings), labels

def load_data(features_extractor):
    dataset = datasets.ImageFolder(DRIVER_PATH)
    embeddings, labels = dataset_to_embeddings(dataset, features_extractor)
    return embeddings, labels, dataset.class_to_idx

def train(embeddings, labels):
    softmax = LogisticRegression(solver='lbfgs', multi_class='multinomial', C=10, max_iter=10000)
    clf = softmax
    clf.fit(embeddings, labels)

    return clf

def draw_bb_on_img(faces, img):
    draw = ImageDraw.Draw(img)
    fs = max(20, round(img.size[0] * img.size[1] * 0.000005))
    font = ImageFont.load_default()#ImageFont.truetype('fonts/font.ttf', fs)
    margin = 5

    for face in faces:
        text = "%s %.2f%%" % (face.top_prediction.label.upper(), face.top_prediction.confidence * 100)
        text_size = font.getsize(text)

        # bounding box
        draw.rectangle(
            (
                (int(face.bb.left), int(face.bb.top)),
                (int(face.bb.right), int(face.bb.bottom))
            ),
            outline='green',
            width=2
        )

        # text background
        draw.rectangle(
            (
                (int(face.bb.left - margin), int(face.bb.bottom) + margin),
                (int(face.bb.left + text_size[0] + margin), int(face.bb.bottom) + text_size[1] + 3 * margin)
            ),
            fill='black'
        )

        # text
        draw.text(
            (int(face.bb.left), int(face.bb.bottom) + 2 * margin),
            text,
            font=font
        )
    return draw

if __name__ == '__main__':
    features_extractor = FaceFeaturesExtractor()
    embeddings, labels, class_to_idx = load_data(features_extractor)
    clf = train(embeddings, labels)

    idx_to_class = {v: k for k, v in class_to_idx.items()}

    target_names = map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0]))
    print(metrics.classification_report(labels, clf.predict(embeddings), target_names=list(target_names)))

    if not os.path.isdir(MODEL_DIR_PATH):
        os.mkdir(MODEL_DIR_PATH)
    model_path = os.path.join('model', 'face_recogniser.pkl')
    joblib.dump(FaceRecogniser(features_extractor, clf, idx_to_class), model_path)

# """## Number plate detection"""

# import pytesseract
# import cv2

# def cleanup_text(text):
	# # strip out non-ASCII text so we can draw the text on the image
  # # using OpenCV
  # text = text.replace('\n', ' ')
  # return "".join([c if ord(c) < 128 else "" for c in text]).strip()

# img = cv2.imread('/content/drive/MyDrive/driver_plate_identify/Number_Plate_dataset/Abdul Hanan.jpeg',cv2.IMREAD_COLOR)
# text = pytesseract.image_to_string(img, config='-c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 --psm 11')

# text

# cleanup_text(text)